{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General parameters\n",
    "\n",
    "OUTPUT_FILE = \"data/output.txt\"\n",
    "\n",
    "SEED = 42\n",
    "TRAIN_SIZE = 0.80 # Percentage of data used in training\n",
    "BATCH_SIZE = 5\n",
    "NUM_BATCHES = 20000 # Character-based tokenization\n",
    "NUM_EPOCHS = 10 # Subword-based tokenization\n",
    "LR = 3e-4 # Learning rate\n",
    "LOSS_SAMPLE_SIZE = 50\n",
    "\n",
    "EMBEDDING_SIZE = 384 # Embedding vector length\n",
    "CONTEXT_SIZE = 384 # Context window size                             \n",
    "FFN_SIZE = 8 # Feed Forward Network size                            \n",
    "HEADS = 8 # Transformer heads in multihead attention                # 4 8\n",
    "BLOCKS = 8 # Transformer blocks (multhead, ffn, ln) depth           # 4 8 \n",
    "\n",
    "top_k = 10 # Used in GPT.generate(..., sampling=\"top_k\") to sample using top_k method.\n",
    "\n",
    "model_filename = (\n",
    "    f\"model_TS{int(TRAIN_SIZE*100)}_BS{BATCH_SIZE}_NB{NUM_BATCHES}_EP_{NUM_EPOCHS}_\"\n",
    "    f\"LR{LR}_ES{EMBEDDING_SIZE}_CS{CONTEXT_SIZE}_\"\n",
    "    f\"FS{FFN_SIZE}_H{HEADS}_B{BLOCKS}.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE= cuda\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"DEVICE=\", device)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "### (1) Simple drama format\n",
    "### (2) (1) with opera tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Simple drama format\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/data.csv\")\n",
    "\n",
    "with open(\"data/text_simple.txt\", \"w\") as f_out:\n",
    "    for i in range(df.shape[0] - 1):\n",
    "        data = df.iloc[i]\n",
    "        next_data = df.iloc[i + 1]\n",
    "\n",
    "        f_out.write(data[\"PlayerLine\"] + \"\\n\")\n",
    "        if data[\"PlayerLinenumber\"] != next_data[\"PlayerLinenumber\"] and not pd.isna(next_data[\"Player\"]):\n",
    "            f_out.write(\"\\n\" + next_data[\"Player\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: More datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "### (1) Character level\n",
    "### (2) Subword level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Character level\n",
    "\n",
    "data = None\n",
    "vocab = None\n",
    "\n",
    "with open(\"data/text_simple.txt\") as f_in:\n",
    "    data = f_in.read()\n",
    "    print(f\"Dataset size {len(data)} characters\")\n",
    "    vocab = list(set(data))\n",
    "    print(f\"Corpus vocabulary size: {len(vocab)}\")\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Lookup table generator\n",
    "\n",
    "char_to_idx = {ch:idx for idx, ch in enumerate(vocab)}\n",
    "idx_to_char = {idx:ch for idx, ch in enumerate(vocab)}\n",
    "str2tokens = lambda str: [char_to_idx[ch] for ch in str]\n",
    "tokens2str = lambda tokens: \"\".join([idx_to_char[token] for token in tokens])\n",
    "\n",
    "import torch\n",
    "tokenized_corpus = torch.tensor(str2tokens(data), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "save_dir = 'saves'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "tokenized_data_path = os.path.join(save_dir, 'tokenized_corpus.pkl')\n",
    "\n",
    "def data_iterator(file_path):\n",
    "    with open(file_path, \"r\") as f_in:\n",
    "        lines = [line.strip() for line in f_in if line.strip()]\n",
    "    return lines\n",
    "\n",
    "if os.path.exists(tokenized_data_path):\n",
    "    print(\"Loading tokenized data from file...\")\n",
    "    with open(tokenized_data_path, 'rb') as f:\n",
    "        tokenized_corpus = pickle.load(f)\n",
    "else:\n",
    "    print(\"Tokenized data not found. Processing and saving...\")\n",
    "    lines = data_iterator(\"data/text_simple.txt\")\n",
    "    tokenized_corpus = [tokenizer.encode(line) for line in lines]\n",
    "    with open(tokenized_data_path, 'wb') as f:\n",
    "        pickle.dump(tokenized_corpus, f)\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "max_length = CONTEXT_SIZE + 1\n",
    "padded_corpus = []\n",
    "for seq in tokenized_corpus:\n",
    "    if len(seq) > max_length:\n",
    "        padded_seq = seq[:max_length]\n",
    "    else:\n",
    "        padded_seq = seq + [tokenizer.pad_token_id] * (max_length - len(seq))\n",
    "    padded_corpus.append(padded_seq)\n",
    "\n",
    "\n",
    "tokenized_corpus = torch.tensor(padded_corpus)\n",
    "print(f\"Tensor shape: {tokenized_corpus.shape}\")\n",
    "print(tokenized_corpus[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenized data from file...\n",
      "Tensor shape: torch.Size([1599, 385])\n",
      "tensor([[10659,   314,   198,  ...,  1309,   502,  3285],\n",
      "        [  198,  5189,   345,  ...,   286,  1123,  9260],\n",
      "        [  198, 13056,    86,  ...,  6465,    44,  1581],\n",
      "        ...,\n",
      "        [ 1921,   198,    44,  ...,  1544,   925,   257],\n",
      "        [  698,  8023,   269,  ...,   465,  1266,    25],\n",
      "        [  290,   994,  3197,  ...,    43,  3963,   360]])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "CONTEXT_SIZE = 384\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "save_dir = 'saves'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "tokenized_data_path = os.path.join(save_dir, 'tokenized_corpus_full.pkl')\n",
    "\n",
    "def read_full_text(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f_in:\n",
    "        text = f_in.read().strip()\n",
    "    return text\n",
    "\n",
    "if os.path.exists(tokenized_data_path):\n",
    "    print(\"Loading tokenized data from file...\")\n",
    "    with open(tokenized_data_path, 'rb') as f:\n",
    "        tokenized_corpus = pickle.load(f)\n",
    "else:\n",
    "    print(\"Tokenized data not found. Processing and saving...\")\n",
    "    full_text = read_full_text(\"data/text_simple.txt\")\n",
    "    tokenized_text = tokenizer.encode(full_text)\n",
    "    print(f\"Total tokens in file: {len(tokenized_text)}\")\n",
    "    \n",
    "    tokenized_corpus = []\n",
    "    step = CONTEXT_SIZE + 1\n",
    "    for i in range(0, len(tokenized_text), step):\n",
    "        segment = tokenized_text[i: i + step]\n",
    "        tokenized_corpus.append(segment)\n",
    "    \n",
    "    with open(tokenized_data_path, 'wb') as f:\n",
    "        pickle.dump(tokenized_corpus, f)\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "max_length = CONTEXT_SIZE + 1\n",
    "padded_corpus = []\n",
    "for seq in tokenized_corpus:\n",
    "    if len(seq) > max_length:\n",
    "        padded_seq = seq[:max_length]\n",
    "    else:\n",
    "        padded_seq = seq + [tokenizer.pad_token_id] * (max_length - len(seq))\n",
    "    padded_corpus.append(padded_seq)\n",
    "\n",
    "tokenized_corpus_tensor = torch.tensor(padded_corpus)\n",
    "print(f\"Tensor shape: {tokenized_corpus_tensor.shape}\")\n",
    "print(tokenized_corpus_tensor[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data split (train/val)\n",
    "\n",
    "train_partition = int(TRAIN_SIZE*len(tokenized_corpus))\n",
    "train = tokenized_corpus[:train_partition]\n",
    "val = tokenized_corpus[train_partition:]\n",
    "\n",
    "print(train[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom nn.Linear :)\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_size,  out_size, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_size, in_size))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_size))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x @ self.weight.T + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom LayerNorm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, int):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "        if self.elementwise_affine:\n",
    "            self.weight = nn.Parameter(torch.ones(self.normalized_shape))\n",
    "            self.bias = nn.Parameter(torch.zeros(self.normalized_shape))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        x_normalized = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        if self.elementwise_affine:\n",
    "            x_normalized = self.weight * x_normalized + self.bias\n",
    "        return x_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Head\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, headspace):\n",
    "        super().__init__()\n",
    "\n",
    "        self.query = Linear(EMBEDDING_SIZE, headspace)\n",
    "        self.key = Linear(EMBEDDING_SIZE, headspace)\n",
    "        self.value = Linear(EMBEDDING_SIZE, headspace)\n",
    "\n",
    "        causal_mask = torch.triu(torch.ones(CONTEXT_SIZE, CONTEXT_SIZE), diagonal=1)\n",
    "        self.register_buffer('mask', causal_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, emb_len = x.shape\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "\n",
    "        att_weights = query @ key.transpose(-2, -1) * (1 / torch.sqrt(torch.tensor(emb_len, dtype=torch.float32)))\n",
    "        att_weights = att_weights.masked_fill(self.mask[:seq_len, :seq_len] == 1, float('-inf'))\n",
    "        att_weights = F.softmax(att_weights, -1)\n",
    "\n",
    "        value = self.value(x)\n",
    "        return att_weights @ value\n",
    "\n",
    "\n",
    "# Demo\n",
    "# att_head = AttentionHead(16, 10, 10)\n",
    "# print(att_head.forward(torch.Tensor(64, 5, 16)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multihead Attention\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        headspace = EMBEDDING_SIZE // HEADS\n",
    "        self.heads = nn.ModuleList([AttentionHead(headspace) for _ in range(HEADS)])\n",
    "        self.linear = Linear(EMBEDDING_SIZE, EMBEDDING_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# mta = MultiHeadAttention(2)\n",
    "# x = torch.rand(1, 7, EMBEDDING_SIZE)\n",
    "# print(mta.forward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Feed Forward Module\n",
    "\n",
    "class FFN_Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            Linear(EMBEDDING_SIZE, EMBEDDING_SIZE * FFN_SIZE),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Linear(EMBEDDING_SIZE * FFN_SIZE, EMBEDDING_SIZE)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer block \n",
    "# Multihead, FFN, LN (Pre-LN)\n",
    "\n",
    "# TODO: CustomLayerNorm\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln = LayerNorm(EMBEDDING_SIZE)\n",
    "        self.causal_attention = MultiHeadAttention()\n",
    "        self.ffn = FFN_Block()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_norm0 = self.ln(x)\n",
    "        x = x + self.causal_attention(x_norm0)\n",
    "\n",
    "        x_norm1 = self.ln(x)\n",
    "        x = x + self.ffn(x_norm1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Model with token_embeds for single char tokenization\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "def get_sinusoidal_positional_encoding(seq_len, embedding_dim, device='cpu'):\n",
    "    position = torch.arange(seq_len, dtype=torch.float, device=device).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, embedding_dim, 2, dtype=torch.float, device=device) * \n",
    "                         -(math.log(10000.0) / embedding_dim))\n",
    "    pe = torch.zeros(seq_len, embedding_dim, device=device)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embeds = nn.Embedding(vocab_size, EMBEDDING_SIZE)\n",
    "        self.pos_embeds = nn.Embedding(CONTEXT_SIZE, EMBEDDING_SIZE)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock() for _ in range(BLOCKS)])\n",
    "        self.ln = LayerNorm(EMBEDDING_SIZE)\n",
    "        self.last_embeddings = Linear(EMBEDDING_SIZE, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        tok_emb = self.token_embeds(x)\n",
    "        pos_emb = get_sinusoidal_positional_encoding(seq_len, EMBEDDING_SIZE, device=x.device)\n",
    "        pos_emb = pos_emb.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.last_embeddings(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            logits_flat = logits.view(batch_size * seq_len, -1)\n",
    "            targets_flat = targets.view(-1).long()\n",
    "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    \n",
    "    def generate(self, inputs, length, sampling=\"argmax\"):\n",
    "        for _ in range(length):\n",
    "            _inputs = inputs[:, -CONTEXT_SIZE:]\n",
    "            logits, _ = self.forward(_inputs)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            if sampling == \"argmax\":\n",
    "                next_input = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "            elif sampling == \"top-k\":\n",
    "                top_k_probs, top_k_indices = torch.topk(probs, k=top_k, dim=-1)\n",
    "                top_k_probs = top_k_probs / torch.sum(top_k_probs, dim=-1, keepdim=True)\n",
    "                next_input = torch.multinomial(top_k_probs, num_samples=1)\n",
    "                next_input = torch.gather(top_k_indices, dim=-1, index=next_input)\n",
    "                \n",
    "            inputs = torch.cat((inputs, next_input), dim=1)\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(data):\n",
    "    selected = torch.randint(len(data) - CONTEXT_SIZE, (BATCH_SIZE,))\n",
    "    input    = torch.stack([data[i:i+CONTEXT_SIZE] for i in selected])\n",
    "    target   = torch.stack([data[i+1:i+CONTEXT_SIZE+1] for i in selected])\n",
    "\n",
    "    return input.to(device), target.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torcheval.metrics import Perplexity\n",
    "import wandb\n",
    "from alive_progress import alive_bar\n",
    "\n",
    "wandb.login(key=\"0d5701acf04c9f9aaafad15365088656730ee934\")\n",
    "wandb.init(project=\"gpt-training\")\n",
    "\n",
    "model = GPT().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "perplexity_metric = Perplexity(device=device)\n",
    "\n",
    "with alive_bar(NUM_BATCHES, title=\"Training Progress\", bar=\"blocks\", spinner=\"dots_waves\") as bar:\n",
    "    for epoch in range(NUM_BATCHES):\n",
    "        model.train()\n",
    "        input, target = batch(train)\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        logits, loss = model(input, target)\n",
    "        \n",
    "        perplexity_metric.update(logits, target)\n",
    "        current_perplexity = perplexity_metric.compute().item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": loss.item(),\n",
    "            \"train_perplexity\": current_perplexity\n",
    "        })\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_input, val_target = batch(val)\n",
    "                val_logits, val_loss = model(val_input, val_target)\n",
    "                perplexity_metric.update(val_logits, val_target)\n",
    "                val_perplexity = perplexity_metric.compute().item()\n",
    "\n",
    "                wandb.log({\n",
    "                    \"val_loss\": val_loss.item(),\n",
    "                    \"val_perplexity\": val_perplexity\n",
    "                })\n",
    "\n",
    "                print(f\"Validation: ðŸ”µ Val Loss = {val_loss.item():.5f} | \"\n",
    "                    f\"ðŸ”´ Val Perplexity = {val_perplexity:.5f}\")\n",
    "        \n",
    "            print(f\"Epoch {epoch:04}/{NUM_BATCHES}: \"\n",
    "                  f\"ðŸŸ¢ Train Loss = {loss.item():.5f} | \"\n",
    "                  f\"ðŸ”µ Train Perplexity = {current_perplexity:.5f}\")\n",
    "\n",
    "        bar()\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(os.getcwd(), \"saves\", model_filename))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (Subword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from alive_progress import alive_bar\n",
    "from transformers import GPT2Tokenizer\n",
    "import wandb\n",
    "import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenized_corpus, context_size):\n",
    "        self.tokenized_corpus = tokenized_corpus\n",
    "        self.context_size = context_size\n",
    "        self.data = []\n",
    "        print(len(self.tokenized_corpus))\n",
    "        for seq in self.tokenized_corpus:\n",
    "            if len(seq) > context_size + 1:\n",
    "                for i in range(0, len(seq) - context_size, context_size):\n",
    "                    input_seq = seq[i : i + context_size]\n",
    "                    target_seq = seq[i + 1 : i + context_size + 1]\n",
    "                    if len(input_seq) == context_size and len(target_seq) == context_size:\n",
    "                        self.data.append((torch.tensor(input_seq, dtype=torch.long),\n",
    "                                          torch.tensor(target_seq, dtype=torch.long)))\n",
    "            elif len(seq) == context_size + 1:\n",
    "                input_seq = seq[:context_size]\n",
    "                target_seq = seq[1:]\n",
    "                self.data.append((torch.tensor(input_seq, dtype=torch.long),\n",
    "                                  torch.tensor(target_seq, dtype=torch.long)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "full_dataset = TextDataset(tokenized_corpus, CONTEXT_SIZE)\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(TRAIN_SIZE * total_size)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "dataloader_val = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "\n",
    "wandb.login(key=\"0d5701acf04c9f9aaafad15365088656730ee934\")\n",
    "wandb.init(project=\"gpt-training\", name=model_filename)\n",
    "\n",
    "model = GPT().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "perplexity_metric = torchmetrics.Perplexity().to(device)\n",
    "\n",
    "\n",
    "with alive_bar(NUM_EPOCHS, title=\"Training Progress\", bar=\"blocks\", spinner=\"dots_waves\") as bar:\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader_train):\n",
    "            model.train()\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            logits, loss = model(inputs, targets)\n",
    "\n",
    "            perplexity_metric.update(logits, targets)\n",
    "            current_perplexity = perplexity_metric.compute().item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": loss.item(),\n",
    "                \"train_perplexity\": current_perplexity\n",
    "            })\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_inputs, val_targets = next(iter(dataloader_val))\n",
    "                    val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "                    val_logits, val_loss = model(val_inputs, val_targets)\n",
    "\n",
    "                    perplexity_metric.update(val_logits, val_targets)\n",
    "                    val_perplexity = perplexity_metric.compute().item()\n",
    "\n",
    "                    wandb.log({\n",
    "                        \"val_loss\": val_loss.item(),\n",
    "                        \"val_perplexity\": val_perplexity\n",
    "                    })\n",
    "\n",
    "                    print(f\"Validation: ðŸ”µ Val Loss = {val_loss.item():.5f} | \"\n",
    "                        f\"ðŸ”´ Val Perplexity = {val_perplexity:.5f}\")\n",
    "\n",
    "                print(f\"Epoch {epoch:02}/{NUM_EPOCHS}: \"\n",
    "                    f\"ðŸŸ¢ Train Loss = {loss.item():.5f} | \"\n",
    "                    f\"ðŸ”µ Train Perplexity = {current_perplexity:.5f}\")\n",
    "\n",
    "            bar()\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(os.getcwd(), \"saves\", model_filename))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "saves_dir = \"saves\"\n",
    "models = [f for f in os.listdir(saves_dir) if f.endswith(\".pth\")]\n",
    "\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=models,\n",
    "    description=\"Model:\",\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "display(dropdown)\n",
    "\n",
    "button = widgets.Button(description=\"Load Model\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def load_model(b):\n",
    "    global model, device\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        selected_model_path = os.path.join(saves_dir, dropdown.value)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        try:\n",
    "            model = GPT().to(device)\n",
    "            checkpoint = torch.load(selected_model_path, map_location=device, weights_only=True)\n",
    "            if isinstance(checkpoint, dict) and \"state_dict\" in checkpoint:\n",
    "                model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "            else:\n",
    "                model.load_state_dict(checkpoint)\n",
    "            model.eval()\n",
    "            print(f\"Loaded model (state_dict): {dropdown.value}\")\n",
    "        except Exception as e:\n",
    "            print(\"Error loading model state dict:\", e)\n",
    "\n",
    "button.on_click(load_model)\n",
    "display(button, output)\n",
    "\n",
    "def generate_text():\n",
    "    if 'model' not in globals():\n",
    "        print(\"Please load a model first!\")\n",
    "        return\n",
    "\n",
    "    OUTPUT_FILE = f\"data/{model_filename}__top-k={top_k}.txt\"\n",
    "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        with tqdm(total=5000, desc=\"Generating Text\", unit=\"token\") as pbar:\n",
    "            for _ in range(5000):\n",
    "                x = model.generate(context, length=1, sampling=\"top-k\")\n",
    "                context = torch.cat((context, x[:, -1:]), dim=1)\n",
    "                pbar.update(1)\n",
    "\n",
    "        generated_text = tokens2str(context[0].tolist())\n",
    "        print(\"Generated Text:\")\n",
    "        print(generated_text)\n",
    "        f.write(generated_text)\n",
    "        print(f\"\\nGenerated text saved to {OUTPUT_FILE}\")\n",
    "\n",
    "generate_button = widgets.Button(description=\"Generate Text\")\n",
    "generate_button.on_click(lambda b: generate_text())\n",
    "display(generate_button)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper block to load the model :)\n",
    "import re\n",
    "\n",
    "#   TS(\\d+)   captures the TRAIN_SIZE*100 as an integer\n",
    "#   BS(\\d+)   captures BATCH_SIZE\n",
    "#   NB(\\d+)   captures NUM_BATCHES\n",
    "#   LR(\\d+(?:\\.\\d+)?) captures LR (integer or decimal)\n",
    "#   ES(\\d+)   captures EMBEDDING_SIZE\n",
    "#   CS(\\d+)   captures CONTEXT_SIZE\n",
    "#   FS(\\d+)   captures FFN_SIZE\n",
    "#   H(\\d+)    captures HEADS\n",
    "#   B(\\d+)    captures BLOCKS\n",
    "pattern = re.compile(\n",
    "    r\"model_TS(\\d+)_BS(\\d+)_NB(\\d+)_EP_(\\d+)_LR(\\d+(?:\\.\\d+)?)_ES(\\d+)_CS(\\d+)_FS(\\d+)_H(\\d+)_B(\\d+)\\.pth\"\n",
    ")\n",
    "\n",
    "def parse_model_filename(filename: str):\n",
    "\n",
    "    match = pattern.match(filename)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Filename '{filename}' does not match the expected pattern.\")\n",
    "\n",
    "    (\n",
    "        ts_str,     # TS(\\d+)\n",
    "        bs_str,     # BS(\\d+)\n",
    "        nb_str,     # NB(\\d+)\n",
    "        ep_str,     # EP(\\d+)\n",
    "        lr_str,     # LR(\\d+(?:\\.\\d+)?)\n",
    "        es_str,     # ES(\\d+)\n",
    "        cs_str,     # CS(\\d+)\n",
    "        fs_str,     # FS(\\d+)\n",
    "        h_str,      # H(\\d+)\n",
    "        b_str       # B(\\d+)\n",
    "    ) = match.groups()\n",
    "\n",
    "    ts_val = int(ts_str)             # This was int(TRAIN_SIZE * 100)\n",
    "    bs_val = int(bs_str)\n",
    "    nb_val = int(nb_str)\n",
    "    ep_val = int(ep_str)\n",
    "    lr_val = float(lr_str)\n",
    "    es_val = int(es_str)\n",
    "    cs_val = int(cs_str)\n",
    "    fs_val = int(fs_str)\n",
    "    h_val  = int(h_str)\n",
    "    b_val  = int(b_str)\n",
    "\n",
    "    train_size = ts_val / 100.0\n",
    "\n",
    "    return {\n",
    "        \"TRAIN_SIZE\": train_size,\n",
    "        \"BATCH_SIZE\": bs_val,\n",
    "        \"NUM_BATCHES\": nb_val,\n",
    "        \"NUM_EPOCHS\": ep_val,\n",
    "        \"LR\": lr_val,\n",
    "        \"EMBEDDING_SIZE\": es_val,\n",
    "        \"CONTEXT_SIZE\": cs_val,\n",
    "        \"FFN_SIZE\": fs_val,\n",
    "        \"HEADS\": h_val,\n",
    "        \"BLOCKS\": b_val\n",
    "    }\n",
    "\n",
    "\n",
    "model_filename = \"model_TS80_BS5_NB20000_EP_10_LR0.0003_ES384_CS384_FS8_H8_B8.pth\"\n",
    "parsed_values = parse_model_filename(model_filename)\n",
    "\n",
    "TRAIN_SIZE      = parsed_values[\"TRAIN_SIZE\"]\n",
    "BATCH_SIZE      = parsed_values[\"BATCH_SIZE\"]\n",
    "NUM_BATCHES     = parsed_values[\"NUM_BATCHES\"]\n",
    "NUM_EPOCHS      = parsed_values[\"NUM_EPOCHS\"]\n",
    "LR              = parsed_values[\"LR\"]\n",
    "EMBEDDING_SIZE  = parsed_values[\"EMBEDDING_SIZE\"]\n",
    "CONTEXT_SIZE    = parsed_values[\"CONTEXT_SIZE\"]\n",
    "FFN_SIZE        = parsed_values[\"FFN_SIZE\"]\n",
    "HEADS           = parsed_values[\"HEADS\"]\n",
    "BLOCKS          = parsed_values[\"BLOCKS\"]\n",
    "\n",
    "print(\"Parsed and assigned:\")\n",
    "print(\"TRAIN_SIZE:     \", TRAIN_SIZE)\n",
    "print(\"BATCH_SIZE:     \", BATCH_SIZE)\n",
    "print(\"NUM_BATCHES:    \", NUM_BATCHES)\n",
    "print(\"NUM_EPOCHS:     \", NUM_EPOCHS)\n",
    "print(\"LR:             \", LR)\n",
    "print(\"EMBEDDING_SIZE: \", EMBEDDING_SIZE)\n",
    "print(\"CONTEXT_SIZE:   \", CONTEXT_SIZE)\n",
    "print(\"FFN_SIZE:       \", FFN_SIZE)\n",
    "print(\"HEADS:          \", HEADS)\n",
    "print(\"BLOCKS:         \", BLOCKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0613228057f4f31a23a9a62b464faa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', layout=Layout(width='50%'), options=('model_TS80_BS5_NB20000_EP_10_LR0.0003_ES3â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92277cbdff144df8b37531aee6f7f793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Load Model', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deddcb9241b645848392135192586503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570c5d72529d4c4fae25c4cae9e88ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate Text', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee53e9661c5a4a48a842b2a579f40645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Text:   0%|          | 0/5000 [00:00<?, ?token/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "!\n",
      "\n",
      "HAMLET\n",
      "I, it, sir.\n",
      "\n",
      "\n",
      "HAMLET\n",
      "That's one of me, this, I do not, but yet\n",
      "this, it as that which ever of mine honour,\n",
      "ithee, that the more, it go to the\n",
      "and the most capital crimes were\n",
      "great, I will follow you, for the\n",
      "the hall of them that cannot be pit of the\n",
      "were out of them that ever kept the\n",
      "his recognubber may read mine is done, as oft,\n",
      "favoury upon the rich men that I may, which if\n",
      "of.\n",
      "\n",
      "Ghost\n",
      "What would not, but I am so, I had not, I had not, who\n",
      "to extend to have them up. But, I am much, if you, my\n",
      "and more, in them that I would put your\n",
      "that your own. An I had\n",
      "inish your ways or, and, I have\n",
      "my son to say nothing but in this matter. I do it\n",
      "to me the love a thing. I have\n",
      "and altogether given me, in the love in the better for\n",
      "guts for the titheth, and in my\n",
      "gods and twenty years, for the better of your\n",
      "of the very soul: but in this\n",
      "their courage that were your own rede. I have made to make you.\n",
      "Enter ROSALIND\n",
      "\n",
      "SILIP\n",
      "Come, come, I do you are too hard, and so much\n",
      "bearded beetle, for both me: and say what\n",
      "the spirit of love is this that\n",
      "and the chain: for the duke's\n",
      "of my house.\n",
      "\n",
      "ORLANDO\n",
      "Why, that, come, let me this is no?\n",
      "\n",
      "Sblood, I have I can scarce tell you\n",
      "would come, I would have ever.\n",
      "\n",
      "ROSALIND\n",
      "I pray you well, what more of my love, therefore truly, which I have some\n",
      "come but that I have made me, I know my\n",
      "have more. Go the devil.\n",
      "\n",
      "ROSALIND\n",
      "ORLANDO\n",
      "I do not old sister, if your knowledge, for the more truly, he is this\n",
      "of mine, that, to your eyes of all: he is no further, if\n",
      "that, there's no mattering a most humorous sadness.\n",
      "\n",
      "ROSALIND\n",
      "I do, I do, and that your wife's one of all men say your\n",
      "with me, to me, and I have your\n",
      "in of you to you to your honourable?\n",
      "\n",
      "ORLANDO\n",
      "I do, sister, an't please you must be not, but one of you are all together,\n",
      "with me like one of love, for your wisdom.\n",
      "\n",
      "ORLANDO\n",
      "I will not what you not this shepherd's no more than in my sister!\n",
      "\n",
      "ORLANDO\n",
      "I thank you well in Rome, but my love me, it.\n",
      "\n",
      "ROSALIND\n",
      "And I not what is no more than in my sister?\n",
      "\n",
      "ORLANDO\n",
      "You'll thank thee, my lord.\n",
      "\n",
      "ROSALIND\n",
      "I am your eyes in this slave, if you, but it not the\n",
      "that I will not your husband's a better part,\n",
      "for your suit to you well.\n",
      "\n",
      "ORLANDO\n",
      "Go, I your husband, and be it your own house.\n",
      "\n",
      "ORLANDO\n",
      "I do you well, Rosalind, I shall see else, for your love me.\n",
      "\n",
      "ROSALIND\n",
      "Why, sister! how shall be you out of my words are my love!\n",
      "\n",
      "ROSALIND\n",
      "No: 'tis I will you well then, sister.\n",
      "\n",
      "ROSALIND\n",
      "I do, my faith, my lord, Rosal them all my heart, that they are all that you\n",
      "be my lord, if you well.\n",
      "\n",
      "CELIA\n",
      "O, my sister.\n",
      "Exit Rosalind is it that?\n",
      "\n",
      "ROSALIND\n",
      "I think I be your Rosalind?\n",
      "ORLANDO\n",
      "ORLANDO\n",
      "I do, my sister. There's a good will you well, Rosalind?\n",
      "\n",
      "ORLANDO\n",
      "ROSALIND\n",
      "I do.\n",
      "ROSALIND\n",
      "ROSALIND\n",
      "Why, if I will, for they will not your Rosalind?\n",
      "\n",
      "ROSALIND\n",
      "You must I never gave it more than a good?\n",
      "\n",
      "OLIVER\n",
      "Marry, 'fore.\n",
      "\n",
      "TOUCHSTONE\n",
      "I am an it more.\n",
      "\n",
      "ROSALIND\n",
      "Nay, my sister, sweet queen.\n",
      "\n",
      "ROSALIND\n",
      "I do, sweet shepherd, sweet: I pray you must be\n",
      "be many weights'Faith, and give me\n",
      "and the Rosal sign, and my sister.\n",
      "\n",
      "ROSALIND\n",
      "Nay, my lord, Rosalind.\n",
      "\n",
      "ROSALIND\n",
      "I do, sister.\n",
      "\n",
      "ROSALIND\n",
      "I do, my trothousand pound a woman, I can, for you, for the\n",
      "that he is this forest.\n",
      "\n",
      "CELIA\n",
      "I pray you call me in your own doors: he is no more.\n",
      "\n",
      "CELIA\n",
      "But he comes the forest, Rosalind?\n",
      "\n",
      "ROSALIND\n",
      "So much she's all, my lord, and I will you.\n",
      "\n",
      "ROSALIND\n",
      "Then I will I, for I will take you all, and I\n",
      "speak no more myself, and I can give me.\n",
      "\n",
      "ROSALIND\n",
      "I would be your lad, my heart.\n",
      "\n",
      "ROSALIND\n",
      "But I will you, sister.\n",
      "\n",
      "ROSALIND\n",
      "I am your wisdom.\n",
      "\n",
      "CELIA\n",
      "No, sister.\n",
      "\n",
      "ROSALIND\n",
      "No, if he hath my lord, I can be your Rosalind.\n",
      "Come, he be it be wi' and therefore.\n",
      "Enter SILVIUS\n",
      "\n",
      "CELIA\n",
      "And I be more.\n",
      "\n",
      "\n",
      "ROSALIND\n",
      "I would not what's sake, what would say what's all your house:\n",
      "and though they'll mend it?\n",
      "Yet, I do not how now?\n",
      "And why, what wouldst you out of mine?\n",
      "\n",
      "ROSALIND\n",
      "And well, he did he is to my lord, I must I'll give me,\n",
      "that your husband's no more.\n",
      "\n",
      "ROSALIND\n",
      "I'll do what will well, if it your eyes in the more.\n",
      "\n",
      "\n",
      "ROSALIND\n",
      "I am I do, my lord, he be my lord, when I have some more.\n",
      "\n",
      "OLIVER\n",
      "You have heard it in this forest.\n",
      "\n",
      "CELIA\n",
      "Why, what say so, he comes to my lord?\n",
      "\n",
      "ROSALIND\n",
      "And in my sister.\n",
      "\n",
      "ROSALIND\n",
      "He's better service, he, what would I'll take all men.\n",
      "\n",
      "ROSALIND\n",
      "I'll do.\n",
      "I do, how do you are my heart, he comes.\n",
      "\n",
      "CELIA\n",
      "I do not what shall, he is this was.\n",
      "\n",
      "OLIVER\n",
      "I am I had, what, if he comes.\n",
      "\n",
      "ROSALIND\n",
      "So much, he comes he doth it.\n",
      "\n",
      "ROSALIND\n",
      "You'll never did, he doth my heart, he comes.\n",
      "Exeunt ORLANDO\n",
      "\n",
      "CELIA\n",
      "You shall be more fool! 'tis a word? 'tis he and out,\n",
      "\n",
      "ORLANDO, he comes it with thee, he didst not more\n",
      "ORLANDO\n",
      "with me, they lack, he did.\n",
      "Exit with us!\n",
      "\n",
      "OLIVER\n",
      "He's not a husband, he will I do not now.\n",
      "\n",
      "ROSALIND\n",
      "I have nothing, certainly, but I do dwell in the\n",
      "with me, for a thing. Good night.\n",
      "\n",
      "ROSALIND\n",
      "And what cannot hear you, he comes the duke, but a\n",
      "if you that's no woman?\n",
      "\n",
      "ROSALIND\n",
      "I am an't!\n",
      "\n",
      "ORLANDO\n",
      "You must not like your husband, I have some one of my heart, my heart, I'll have it.\n",
      "\n",
      "RIS\n",
      "Why, sister, Rosal, my faith, my lord, sweet:\n",
      "but that's no less, I will I'll marry thee, sweet maids,\n",
      "but that I know you.\n",
      "\n",
      "ROSALIND\n",
      "You shall, sweet Oliver, my lord of them not?\n",
      "\n",
      "LE BEAU\n",
      "No, what's the priest, but I will, for shame, it is, but one\n",
      "and what they would be not how I do: but my\n",
      "come, my lord, that your husband, for they will make\n",
      "and the it.\n",
      "\n",
      "ROSALIND\n",
      "You shall be you well, the forest.\n",
      "\n",
      "ORLANDO\n",
      "I will I know your eyes in my heart.\n",
      "\n",
      "ORLANDO\n",
      "You do, I will not what means for your own house.\n",
      "\n",
      "ROSALIND\n",
      "I pray you have done!\n",
      "Exe?\n",
      "\n",
      "ORLANDO\n",
      "No, how goes, my lord, that's all that I will I be your\n",
      "cote the priest: therefore it with my\n",
      "tame, I have some more of love, and will\n",
      "that, and in the better temper'd at my verses will give me.\n",
      "Enter ORLANDO\n",
      "\n",
      "\n",
      "OLIVER\n",
      "How now! what would you, what is this is this?\n",
      "\n",
      "\n",
      "Methinks, Rosalready made me to my eldest of mine, the\n",
      "And\n",
      "That I be the particular fancy, the chaff't.\n",
      "\n",
      "ROSALIND\n",
      "I will you, my lord, my sister?\n",
      "If the wrestler, my sister! who's no more it?\n",
      "\n",
      "OLIVER\n",
      "No, this they do you shall receive what is this forest.\n",
      "\n",
      "JAQUES\n",
      "How much of his ecstasy!\n",
      "\n",
      "JAQUES\n",
      "And what follows?\n",
      "ORLANDO\n",
      "ROSALIND\n",
      "He does your Rosalind.\n",
      "\n",
      "ROSALIND\n",
      "I do, Rosalind: he comes the duke of many a\n",
      "And will be the forest.\n",
      "\n",
      "ROSALIND\n",
      "I do you to your wisdom of them now.\n",
      "\n",
      "ORLANDO\n",
      "I think you no more than in my house, for all withalind?\n",
      "\n",
      "ORLANDO\n",
      "And your love!\n",
      "Marry, for your Rosalind is the priest, if it\n",
      "And therefore, a true one that you well.\n",
      "\n",
      "JAQUES\n",
      "That I be you well in your wisdom\n",
      "To tell you well.\n",
      "\n",
      "ORLANDO\n",
      "I do, 'tis your good will well counterfeited! Why that I know your\n",
      "\n",
      "RATES\n",
      "You are all the more fool, and my sister.\n",
      "\n",
      "ORLANDO\n",
      "Come, my lord, I'll give them all together.\n",
      "You shall find you, I will never be my Rosalind:\n",
      "And my eldest son your Rosalind,\n",
      "To the better vantage with me,\n",
      "To show'd with my sister, and will\n",
      "To your will make your honourable men.\n",
      "\n",
      "JAQUES\n",
      "I do your madness, give me,\n",
      "I do protest, and know you,\n",
      "And give me to the very heart.\n",
      "Exeunt\n",
      "ACT II\n",
      "JAQUES\n",
      "I do well on thee, sir.\n",
      "SCENE V. The same. Enter MARK ANTONY's house.\n",
      "Exit Servant\n",
      "\n",
      "WESTMORNER\n",
      "I am an English camp, and be your husband.\n",
      "Enter ANTONY PER comes. Enter MARK ANTONY, BEDFOLKneeling]  WINCHESS\n",
      "\n",
      "How now! why, they all that they do they are you, and that they do?\n",
      "And you well said. Enter BRUTUS\n",
      "\n",
      "Saw their man, they do not now are all?\n",
      "\n",
      "\n",
      "Beneath he doth, they do.\n",
      "Your vile a man, they do you shall receive us in their house before\n",
      "And make 'em here at your good.\n",
      "Enter MARK ANTONY\n",
      "\n",
      "BASSIUS\n",
      "\n",
      "What's away, sweet majesty is my lord, for your general:\n",
      "We are those that are they do your general: but I do,\n",
      "By this they are at the mortal and they will\n",
      "The nature, they are they will\n",
      "And will give them like to, and give them.\n",
      "The thing is the other of our health.\n",
      "Exit Lucilius LAFEU\n",
      "\n",
      "CASCAESARUS\n",
      "I do, my heart, I will I do not now,\n",
      "Makes the utter'd at this present.\n",
      "\n",
      "SIR OLIVER MORTIMOGEN\n",
      "But now, good night.\n",
      "\n",
      "MARK ANTONY\n",
      "WISANLEY\n",
      "I do, good fortune's health, peace!\n",
      "Exeunt all.\n",
      "\n",
      "SCENE III. Another part, alar's the field of Epidius.\n",
      "Enter CLEOP OF Eros, ER, and all:\n",
      "\n",
      "\n",
      "Y\n",
      "You DOLIVER\n",
      "Where's the Vols, my lord?\n",
      "I am I, sir.\n",
      "\n",
      "Sicilius!\n",
      "\n",
      "M\n",
      "Porter\n",
      "I am a sudden, my lord, for we will I do not what end.\n",
      "Enter an end?\n",
      "\n",
      "MARK ANTONY\n",
      "I have some more. O, my lord:\n",
      "And now, my lord, at all, I'll follow.\n",
      "\n",
      "MARK ANTONY\n",
      "I do,\n",
      "Thy, my lord, and give me, and that they are\n",
      "A mother, to my lord, to my lord,\n",
      "And to me, for a word: give them all.\n",
      "Exit ALEXAS\n",
      "\n",
      "MARK ANTONY\n",
      "You'll give me, sir, I know you:\n",
      "And I do't.\n",
      "\n",
      "LEPIDUS\n",
      "Go, come, my lord, we'll give them all.\n",
      "Exeunt.\n",
      "Enter OCTAVIUS\n",
      "\n",
      "MARK ANTONY\n",
      "PIDUS\n",
      "You shall, Antony:\n",
      "If we'll give us.\n",
      "\n",
      "CLEOPATRA\n",
      "You shall do, Antony shall hear thee, Caesar.\n",
      "The time, Antony is't.\n",
      "\n",
      "\n",
      "CHARMIAN\n",
      "Good my lord, Antony.\n",
      "Exit Misen of the stars.\n",
      "\n",
      "M\n",
      "MARK ANTONY\n",
      "The sun's.\n",
      "CLEOP, I have made Caesar.\n",
      "Exit CHARMIAN\n",
      "I do not well pleased you,\n",
      "A\n",
      "But now, my lord,\n",
      "And that they shall command, the will be Cleop to my queen,\n",
      "Of what would best men.\n",
      "\n",
      "MARK ANTONY\n",
      "The way but one.\n",
      "Exit ALEX, come,\n",
      "Exit ALEXAS\n",
      "And Antony, I shall be,\n",
      "So, my fortunes all, he has my Caesar.\n",
      "Exeunt all out of Egypt, the monument! and let's. To the OCTAVIA\n",
      "\n",
      "\n",
      "CHARMIAN\n",
      "I have made me, my Caesar.\n",
      "Exeunt all but Antony.\n",
      "Exit CLEOPATRA\n",
      "\n",
      "CHARMIAN\n",
      "Go, like a soldier, and we will.\n",
      "\n",
      "\n",
      "CLEOPATRA\n",
      "Go home, I do,\n",
      "We are they say so,\n",
      "A noble Caesar.\n",
      "PIDI have a word: pray you.\n",
      "\n",
      "\n",
      "CLEOPATRA\n",
      "So say, pardon, let's.\n",
      "\n",
      "MARK ANTONY\n",
      "Good my lord, I will, I'll give me,\n",
      "And I will make my death.\n",
      "The,\n",
      "\n",
      "My powers I, my fortunes all that's.\n",
      "\n",
      "CHARMIAN\n",
      "So well, I do, and give me.\n",
      "\n",
      "MARK ANTONY\n",
      "Caesar's house, Antony.\n",
      "Exit ALE.\n",
      "Exit ALE, Charmian.\n",
      "Enter MARK ANTONY\n",
      "\n",
      "So, Antony: I will you, Caesar.\n",
      "\n",
      "\n",
      "Messenger\n",
      "So, the queen, my lord, the queen.\n",
      "\n",
      "MARK ANTONY\n",
      "So Fulvia, the Fulvia's sake,\n",
      "The one.\n",
      "\n",
      "MARK ANTONY\n",
      "So Caesar, I have a soldier.\n",
      "\n",
      "CLEOPATRA\n",
      "You'Faith, Egypt, my lord.\n",
      "\n",
      "\n",
      "So, the way, they say I have done't.\n",
      "\n",
      "CLEOPATRA\n",
      "I shall hear thee,--\n",
      "I do,\n",
      "I say, the worm.\n",
      "Exeunt all the OCTAVIUS\n",
      "\n",
      "MARK ANTONY\n",
      "MARK ANTONY\n",
      "I do, they do,\n",
      "So,--\n",
      "\n",
      "O, Cleopatra, they shall never never shall never gave it.\n",
      "Enter CLEOPATRA\n",
      "\n",
      "OCTAVI cannot, Antony.\n",
      "\n",
      "\n",
      "MARK ANTONY\n",
      "Good even to Egypt, Antony, my lord,--\n",
      "\n",
      "MARK ANTONY\n",
      "Go, and all the stars\n",
      "My thoughts, and these same,--\n",
      "\n",
      "\n",
      "CLEOPATRA\n",
      "Go,\n",
      "You'll never loved.\n",
      "MARK ANTONY\n",
      "PIDUS\n",
      "Good.\n",
      "\n",
      "CLEOPATRA\n",
      "I am I do, my lord,--\n",
      "\n",
      "\n",
      "MARK ANTONY\n",
      "I will,--\n",
      "So, they say so,--\n",
      "\n",
      "\n",
      "MARK ANTONY\n",
      "You'll give thee, my lord,\n",
      "That's house: I will bring my blood.\n",
      "\n",
      "MARK ANTONY\n",
      "I will, that's all well, I know, and give her, and know you:\n",
      "A\n",
      "That's well said, she's all I will not, for Cleopatra as I'll never\n",
      "That I be mine, she will give her,\n",
      "And she never shall.\n",
      "\n",
      "MARK ANTONY\n",
      "So please, that's.\n",
      "\n",
      "DOMITIUS CAESAR\n",
      "You'll leave me, 'tis a soldier,\n",
      "To give it to thee.\n",
      "Exit EROS\n",
      "\n",
      "AGRIPPA\n",
      "IUS ENOBARBUS ENOBARBUS\n",
      "A\n",
      "I shall remain your health.\n",
      "\n",
      "MARK ANTONY\n",
      "IUS ENOBARBUS\n",
      "Go to the world, Caesar, Caesar,\n",
      "That I will give me, Antony, and give it\n",
      "By this they do fear me.\n",
      "Exit CLEOPATRA\n",
      "\n",
      "Soldier rejoice in Rome, my noble soldier, and give me,\n",
      "MARK ANTONY\n",
      "MARK ANTONY\n",
      "I mean to you.\n",
      "Exeunt OCTAVIUS CAESAR, Lepidus.\n",
      "\n",
      "DOMITIUS ENOBARBUS ENOBARBUS\n",
      "\n",
      "O, Caesar, that's the great Juno-t\n",
      "And Antony: he is that's the Egypt, he shall find you Caesar.\n",
      "\n",
      "\n",
      "DOMITIUS ENOBARBUS ENOBARBUS\n",
      "MARK ANTONY\n",
      "And Caesar dead.\n",
      "\n",
      "Soldier rejoice Antony, Egypt, Antony, Antony.\n",
      "\n",
      "\n",
      "TH's Caesar, Caesar, Caesar, Caesar. Antony, Octavia.\n",
      "\n",
      "\n",
      "PIDUS ENOBARBUS\n",
      "Caesar,\n",
      "Go, Antony, Caesar, for I am sure\n",
      "To Caesar.\n",
      "\n",
      "DOMITI shall I have made wars.\n",
      "\n",
      "DOMITI have made Caesar.\n",
      "\n",
      "DOMITIUS ENOBARBUS ENOBARBUS\n",
      "And give thee, Antony.\n",
      "\n",
      "MARK ANTONY\n",
      "[To CLEOPATRA\n",
      "To Caesar.\n",
      "\n",
      "PIDUS ENOBARBUS ENOBARBUS\n",
      "Worthy Octavia.\n",
      "\n",
      "\n",
      "OCTAVIUS CAESAR\n",
      "And give thee.\n",
      "\n",
      "\n",
      "MARK ANTONY\n",
      "[Aside to Antony, I be Antony.\n",
      "\n",
      "\n",
      "MARK ANTONY\n",
      "Ca shall.\n",
      "AGRIPPA\n",
      "Go be Antony, Caesar, Antony shall stay, and be.\n",
      "\n",
      "\n",
      "MARK ANTONY\n",
      "[Aside]  The Queen of Caesar.\n",
      "\n",
      "MECAENAS\n",
      "MARK ANTONY\n",
      "[Aside to Antony, Antony, Caesar, Caesar.\n",
      "\n",
      "\n",
      "LEPIDUS ENOBARBUS ENOBARBUS\n",
      "MARK ANTONY\n",
      "I will never be, Antony, she would not,\n",
      "MARK ANTONY\n",
      "And make his heart, it in their graves.\n",
      "\n",
      "OCTAVI say 'tis the pyramid,\n",
      "The cause, Antony, he shall be Antony, the heart,\n",
      "And Antony, the affairs of Caesar:\n",
      "Since Caesar I shall not Caesar.\n",
      "\n",
      "Exit SELEPays it is Antony,\n",
      "To give thee, Antony, Caesar shall hear me.\n",
      "Exit ALEXAS\n",
      "\n",
      "MARK ANTONY\n",
      "Come, I knew you all,\n",
      "To give me, Antony.\n",
      "\n",
      "\n",
      "PIDUS ENOBARBUS\n",
      "The doth Caesar,\n",
      "With Caesar.\n",
      "But Antony, Antony\n",
      "And, Antony, Antony.\n",
      "\n",
      "\n",
      "AGRIPPA\n",
      "[Aside to Rome, for the hills I'll vouch Antony,\n",
      "Shall I am I can never come.\n",
      "\n",
      "\n",
      "MARK ANTONY\n",
      "But Cleopatra, my death were best,\n",
      "That I will never gave it so he did.\n",
      "Exit ALEXAS\n",
      "\n",
      "MARK ANTONY\n",
      "[To Caesar.\n",
      "PIDUS\n",
      "PIDUS\n",
      "He is gone, Antony\n",
      "And let it.\n",
      "To the stars give me, Antony, my fortunes shall be Antony.\n",
      "Exit MARDIANA\n",
      "The queen. My queen. Antony is dead. The messengers!\n",
      "Exeunt Antony. Another room, Cleop, and Antony.\n",
      "SCENE I. Enter OCTAVIUS and MARK ANTONY's house.\n",
      "Enter a Messenger\n",
      "\n",
      "MARK ANTONY\n",
      "Go, Antony, Lepidus.\n",
      "\n",
      "\n",
      "MARK ANTONY\n",
      "We'll leave me, my Antony, Antony, Antony.\n",
      "Exit ALEXAS\n",
      "\n",
      "PIDUS\n",
      "Caesar, Antony, for Caesar:\n",
      "I am thy hand.\n",
      "\n",
      "PIDUS\n",
      "PIDUS\n",
      "The queen, Caesar cannot come, we shall hear me.\n",
      "Exit SELEPIDUS\n",
      "Enter MECAENAS\n",
      "\n",
      "MARK ANTONY\n",
      "The matter\n",
      "Caesar's house, Caesar.\n",
      "\n",
      "\n",
      "MARK ANTONY\n",
      "I say you, Caesar.\n",
      "\n",
      "MARK ANTONY\n",
      "Caesar's house, Caesar.\n",
      "The gods!\n",
      "\n",
      "Exeunt OCTAVIUS\n",
      "\n",
      "All bond, Cleopatra, Antony, Antony: and Antony.\n",
      "\n",
      "OCTAVI am Antony: Antony, Antony, Antony\n",
      "The house: give me to Caesar.\n",
      "Exeunt Antony, the OCTAVIUS\n",
      "\n",
      "MARK ANTONY\n",
      "MARK ANTONY\n",
      "PIDUS\n",
      "I have not theIDUS\n",
      "Where's house:\n",
      "I do you,\n",
      "MARK ANTONY\n",
      "I do, Antony.\n",
      "\n",
      "MENAS\n",
      "PIDUS\n",
      "I shall be Cleopatra, Antony.\n",
      "\n",
      "DOMITIUS\n",
      "He says well pleased, I shall not,\n",
      "MARK ANTONY\n",
      "I do, Antony: Antony\n",
      "He does well, I know you shall be Antony, Caesar, and, Antony.\n",
      "\n",
      "MARK ANTONY\n",
      "MARK ANTONY\n",
      "P\n",
      "\n",
      "Generated text saved to data/model_TS80_BS5_NB20000_EP_10_LR0.0003_ES384_CS384_FS8_H8_B8.pth__top-k=10.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "saves_dir = \"saves\"\n",
    "models = [f for f in os.listdir(saves_dir) if f.endswith(\".pth\")]\n",
    "\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=models,\n",
    "    description=\"Model:\",\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "display(dropdown)\n",
    "\n",
    "button = widgets.Button(description=\"Load Model\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def load_model(b):\n",
    "    global model, device\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        selected_model_path = os.path.join(saves_dir, dropdown.value)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        try:\n",
    "            model = GPT().to(device)\n",
    "            checkpoint = torch.load(selected_model_path, map_location=device, weights_only=True)\n",
    "            if isinstance(checkpoint, dict) and \"state_dict\" in checkpoint:\n",
    "                model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "            else:\n",
    "                model.load_state_dict(checkpoint)\n",
    "            model.eval()\n",
    "            print(f\"Loaded model (state_dict): {dropdown.value}\")\n",
    "        except Exception as e:\n",
    "            print(\"Error loading model state dict:\", e)\n",
    "\n",
    "button.on_click(load_model)\n",
    "display(button, output)\n",
    "\n",
    "def generate_text():\n",
    "    if 'model' not in globals():\n",
    "        print(\"Please load a model first!\")\n",
    "        return\n",
    "\n",
    "    OUTPUT_FILE = f\"data/{model_filename}__top-k={top_k}.txt\"\n",
    "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        with tqdm(total=5000, desc=\"Generating Text\", unit=\"token\") as pbar:\n",
    "            for _ in range(5000):\n",
    "                x = model.generate(context, length=1, sampling=\"top-k\")\n",
    "                context = torch.cat((context, x[:, -1:]), dim=1)\n",
    "                pbar.update(1)\n",
    "\n",
    "        # Use GPT2Tokenizer to decode the tokens.\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "        token_ids = context[0].tolist()\n",
    "        generated_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "        \n",
    "        print(\"Generated Text:\")\n",
    "        print(generated_text)\n",
    "        f.write(generated_text)\n",
    "        print(f\"\\nGenerated text saved to {OUTPUT_FILE}\")\n",
    "\n",
    "\n",
    "generate_button = widgets.Button(description=\"Generate Text\")\n",
    "generate_button.on_click(lambda b: generate_text())\n",
    "display(generate_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm.notebook import tqdm  # Use tqdm for notebooks\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "# nltk.download('punkt_tab') \n",
    "\n",
    "SAVES_DIR = \"saves\"\n",
    "MODEL_FILENAME = \"model_TS80_BS5_NB20000_EP_10_LR0.0003_ES384_CS384_FS8_H8_B8.pth\" \n",
    "MODEL_PATH = os.path.join(SAVES_DIR, MODEL_FILENAME)\n",
    "\n",
    "base_model_name, _ = os.path.splitext(MODEL_FILENAME)\n",
    "REFS_FILE = base_model_name + \"_refs.npy\"\n",
    "HYPS_FILE = base_model_name + \"_hyps.npy\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -------- Load Saved Model --------\n",
    "model = GPT().to(device)\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=True)\n",
    "if isinstance(checkpoint, dict) and \"state_dict\" in checkpoint:\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "else:\n",
    "    model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "\n",
    "# -------- Prepare Validation Dataset --------\n",
    "full_dataset = TextDataset(tokenized_corpus, CONTEXT_SIZE)\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(TRAIN_SIZE * total_size)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "_, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "dataloader_val = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "# -------- Initialize Tokenizer --------\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "# -------- Compute BLEU Score --------\n",
    "smooth = SmoothingFunction().method1\n",
    "\n",
    "if os.path.exists(REFS_FILE) and os.path.exists(HYPS_FILE):\n",
    "    references = np.load(REFS_FILE, allow_pickle=True).tolist()\n",
    "    hypotheses = np.load(HYPS_FILE, allow_pickle=True).tolist()\n",
    "    print(\"Loaded saved references and hypotheses from:\")\n",
    "    print(f\"  {REFS_FILE}\")\n",
    "    print(f\"  {HYPS_FILE}\")\n",
    "else:\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    pbar = tqdm(dataloader_val, desc=\"Processing Batches\", leave=True)\n",
    "    for inputs, targets in pbar:\n",
    "        start_time = time.perf_counter() \n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model.generate(inputs, length=inputs.size(1), sampling=\"top-k\")\n",
    "        \n",
    "        target_ids = targets.tolist()\n",
    "        output_ids = outputs.tolist()\n",
    "        ref_texts = tokenizer.batch_decode(target_ids, skip_special_tokens=True)\n",
    "        hyp_texts = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "        \n",
    "        batch_refs = [[nltk.word_tokenize(ref)] for ref in ref_texts]\n",
    "        batch_hyps = [nltk.word_tokenize(hyp) for hyp in hyp_texts]\n",
    "        \n",
    "        references.extend(batch_refs)\n",
    "        hypotheses.extend(batch_hyps)\n",
    "        \n",
    "        batch_time = time.perf_counter() - start_time\n",
    "        avg_sample_time = batch_time / inputs.size(0)\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'batch_time (s)': f\"{batch_time:.2f}\",\n",
    "            'avg_sample_time (s)': f\"{avg_sample_time:.4f}\"\n",
    "        })\n",
    "    \n",
    "    np.save(REFS_FILE, np.array(references, dtype=object))\n",
    "    np.save(HYPS_FILE, np.array(hypotheses, dtype=object))\n",
    "    print(\"Saved references and hypotheses to:\")\n",
    "    print(f\"  {REFS_FILE}\")\n",
    "    print(f\"  {HYPS_FILE}\")\n",
    "\n",
    "bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smooth)\n",
    "print(\"BLEU score: {:.4f}\".format(bleu_score))\n",
    "\n",
    "bleu_txt_file = base_model_name + \"_bleu.txt\"\n",
    "with open(bleu_txt_file, 'w') as f:\n",
    "    f.write(\"BLEU score: {:.4f}\".format(bleu_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "api = wandb.Api()\n",
    "project_path = \"ldg2875/gpt-training\"\n",
    "runs = api.runs(project_path)\n",
    "\n",
    "data = []\n",
    "\n",
    "for run in runs:\n",
    "    if run.state != \"finished\":\n",
    "        continue\n",
    "    run_summary = run.summary._json_dict\n",
    "    run_config = run.config\n",
    "    run_data = {\n",
    "        \"run_id\": run.id,\n",
    "        \"name\": run.name,\n",
    "        \"state\": run.state,\n",
    "        **run_summary,\n",
    "        **run_config\n",
    "    }\n",
    "    data.append(run_data)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"wandb_run_data.csv\", index=False)\n",
    "df.to_csv(\"wandb_run_data.txt\", index=False, sep=\"\\t\")\n",
    "print(\"Export complete! Files saved as 'wandb_run_data.csv' and 'wandb_run_data.txt'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "csv_file = \"wandb_run_data.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "df['val_loss'] = pd.to_numeric(df['val_loss'], errors='coerce')\n",
    "df['val_perplexity'] = pd.to_numeric(df['val_perplexity'], errors='coerce')\n",
    "df = df.dropna(subset=['val_loss', 'val_perplexity'])\n",
    "best_val_loss = df.sort_values('val_loss').head(5)\n",
    "best_val_perplexity = df.sort_values('val_perplexity').head(5)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(best_val_loss['name'], best_val_loss['val_loss'], color='skyblue')\n",
    "plt.title(\"Top 5 Models by Validation Loss\")\n",
    "plt.xlabel(\"Model Name\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(best_val_perplexity['name'], best_val_perplexity['val_perplexity'], color='salmon')\n",
    "plt.title(\"Top 5 Models by Validation Perplexity\")\n",
    "plt.xlabel(\"Model Name\")\n",
    "plt.ylabel(\"Validation Perplexity\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
